{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge, Lasso and Elastic Net\n",
    "At this point we've seen a number of criteria and algorithms for fitting regression models to data. We've seen the simple linear regression using ordinary least squares, and its more general regression of polynomial functions. We've also seen how we can arbitrarily overfit models to data using kernel methods or feature engineering. With all of that, we began to explore other tools to analyze this general problem of overfitting versus underfitting. This included train and test splits, bias and variance, and cross validation.\n",
    "\n",
    "Now we're going to take a look at another way to tune our models. These methods all modify our mean squared error function that we were optimizing against. The modifications will add a penalty for large coefficient weights in our resulting model. If we think back to our case of feature engineering, we can see how this penalty will help combat our ability to create more accurate models by simply adding additional features.\n",
    "\n",
    "In general, all of these penalties are known as $L^p norms$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $L^p$ norm of x\n",
    "In order to help account for underfitting and overfitting, we often use what are called $L^p$ norms.   \n",
    "The **$L^p$ norm of x** is defined as:  \n",
    "\n",
    "### $||x||_p  =  \\big(\\sum_{i} x_i^p\\big)^\\frac{1}{p}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Ridge (L2)\n",
    "One common normalization is called Ridge Regression and uses the $l_2$ norm (also known as the Euclidean norm) as defined above.   \n",
    "The ridge coefficients minimize a penalized residual sum of squares:    \n",
    "    $ \\sum(\\hat{y}-y)^2 + \\lambda\\bullet w^2$\n",
    "\n",
    "Write this loss function for performing ridge regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_loss(y, y_hat):\n",
    "    return l2_err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Lasso (L1)\n",
    "Another common normalization is called Lasso Regression and uses the $l_1$ norm.   \n",
    "The ridge coefficients minimize a penalized residual sum of squares:    \n",
    "    $ \\sum(\\hat{y}-y)^2 + \\lambda\\bullet |w|$\n",
    "\n",
    "Write this loss function for performing ridge regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso_loss(y, y_hat):\n",
    "    return l1_err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Ridge in Practice\n",
    "Modify your polynomial linear regression function to incorporate the Ridge l2 penalty rather then simply MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Previous Code\n",
    "def grad_desc(x, y, precision, max_iters, w, rss, predict):\n",
    "    previous_step_size = 1 #Arbitrary\n",
    "    iteration = 0 #iteration counter\n",
    "    while (previous_step_size > precision) & (iteration < max_iters):\n",
    "        if iteration%500==0:\n",
    "            print('Iteration {} \\nCurrent weights:\\n{} \\nRSS Produced: {}'.format(iteration, w, rss(y, predict(X, w))))\n",
    "            print('\\n\\n')\n",
    "        #Calculate Nearby Points\n",
    "        sample_steps = np.array(w)/1000.0 #Take mean of feature weights and divide by 100. /\n",
    "                                                #Use this to create surrounding sample points.\n",
    "        #Calculate the Gradient\n",
    "        #Look at weights surrounding our current position.\n",
    "        weights_sample_space = np.array([w+(i*sample_steps) for i in range(-50,51)])\n",
    "\n",
    "        #Calculate the RSS error for this surrounding weights-space.\n",
    "        y_hats = np.array([predict(X, wi) for wi in weights_sample_space])\n",
    "        rss_weights_sample_space = np.array([rss(y, y_hat) for y_hat in y_hats])\n",
    "\n",
    "        #weights_and_y_hats = np.concatenate((weights_sample_space,  np.array([rss_weights_sample_space]).T), axis=1)\n",
    "        gradients = np.gradient(rss_weights_sample_space)\n",
    "        steepest_gradient_idx = max(list(enumerate(gradients)), key=lambda x: x[1])[0]\n",
    "\n",
    "\n",
    "        #Move opposite the gradient by some step size\n",
    "        prev_w = w #Save for calculating how much we moved\n",
    "        w = w - alpha*weights_sample_space[steepest_gradient_idx]\n",
    "\n",
    "        previous_step_size = np.sqrt(sum([wi**2 for wi in w-prev_w]))\n",
    "        iteration += 1\n",
    "    \n",
    "\n",
    "    print(\"Gradient descent converged. Local minimum identified at:\")\n",
    "    print('Iteration {} \\nCurrent weights:\\n{} \\nRSS Produced: {}'.format(iteration, w, rss(y, predict(X, w))))\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your Ridge Linear Regression Function here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Lasso in Practice\n",
    "Modify your polynomial linear regression function to incorporate the Lasso l1 penalty rather then simply MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your Lasso Linear Regression Function here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run + Compare your Results\n",
    "Answer the following questions:\n",
    "* Which model do you think created better results overall? \n",
    "* Comment on the differences between the coefficients of the resulting models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('movie_data_detailed_with_ols.xlsx')\n",
    "df.head()\n",
    "X = df[['budget', 'imdbRating',\n",
    "       'Metascore', 'imdbVotes']]\n",
    "y = df['domgross']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run your models here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your answers here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
